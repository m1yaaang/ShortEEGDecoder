{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWritten by, \\nSriram Ravindran, sriram@ucsd.edu\\n\\nOriginal paper - https://arxiv.org/abs/1611.08024\\n\\nPlease reach out to me if you spot an error.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Written by, \n",
    "Sriram Ravindran, sriram@ucsd.edu\n",
    "\n",
    "Original paper - https://arxiv.org/abs/1611.08024\n",
    "\n",
    "Please reach out to me if you spot an error.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn numpy scipy matplotlib seaborn pandas\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import random\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # [중요] GUI 백엔드가 필요없는 환경에서 사용 (예: 서버)\n",
    "import matplotlib.pyplot as plt\n",
    "import gc # 가비지 컬렉션 모듈 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here's the description from the paper</p>\n",
    "<img src=\"EEGNet.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC input size: 224\n",
      "Input shape: torch.Size([1, 1, 448, 53])\n",
      "Output shape: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, n_channels=53, n_timepoints=448, n_classes=6):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = n_timepoints\n",
    "        self.C = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # [Layer 1] Spatial Conv (공간 필터)\n",
    "        # n_channels개 채널의 정보를 하나로 압축\n",
    "        # Input: (Batch, 1, T, C) -> Output: (Batch, 16, T, 1)\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, n_channels), padding=0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "        \n",
    "        # [Layer 2] Temporal Conv (시간 필터)\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1)) \n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "        # [Layer 3] Depthwise/Separable Conv\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "        # FC Layer 차원 계산\n",
    "        self._calculate_fc_input_size(n_channels, n_timepoints)\n",
    "        \n",
    "        # FC Layer - n_classes 출력 (5-class classification)\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, n_classes)\n",
    "        \n",
    "    def _calculate_fc_input_size(self, n_channels, n_timepoints):\n",
    "        \"\"\"Forward pass를 통해 FC layer 입력 크기 계산\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, 1, n_timepoints, n_channels)\n",
    "            x = F.elu(nn.Conv2d(1, 16, (1, n_channels), padding=0)(x))\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "            x = nn.ZeroPad2d((16, 17, 0, 1))(x)\n",
    "            x = F.elu(nn.Conv2d(1, 4, (2, 32))(x))\n",
    "            x = nn.MaxPool2d(2, 4)(x)\n",
    "            x = nn.ZeroPad2d((2, 1, 4, 3))(x)\n",
    "            x = F.elu(nn.Conv2d(4, 4, (8, 4))(x))\n",
    "            x = nn.MaxPool2d((2, 4))(x)\n",
    "            self.fc_input_size = x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1: Spatial Learning\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, 0.25, training=self.training)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Layer 2: Temporal Learning\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, 0.25, training=self.training)\n",
    "        x = self.pooling2(x)\n",
    "        \n",
    "        # Layer 3: High-level Feature Learning\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, 0.25, training=self.training)\n",
    "        x = self.pooling3(x)\n",
    "        \n",
    "        # FC Layer\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)  # CrossEntropyLoss에서 softmax 처리\n",
    "        return x\n",
    "\n",
    "# 모델 초기화 (53채널, 448 timepoints, 5 classes)\n",
    "net = EEGNet(n_channels=53, n_timepoints=448, n_classes=6).cuda(0)\n",
    "print(f\"FC input size: {net.fc_input_size}\")\n",
    "\n",
    "# 테스트\n",
    "test_input = torch.randn(1, 1, 448, 53).cuda(0)\n",
    "test_output = net(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "\n",
    "# Loss: CrossEntropyLoss (5-class classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate function returns values of different criteria like accuracy, precision etc. \n",
    "In case you face memory overflow issues, use batch size to control how many samples get evaluated at one time. Use a batch_size that is a factor of length of samples. This ensures that you won't miss any samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, params=[\"acc\"]):\n",
    "    \"\"\"Multi-class classification용 evaluate 함수\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, mask in data_loader:\n",
    "            inputs = inputs.cuda(0)\n",
    "            outputs = model(inputs)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    for param in params:\n",
    "        if param == 'acc':\n",
    "            results.append(accuracy_score(all_labels, all_preds))\n",
    "        if param == \"auc\":\n",
    "            # Multi-class AUC (one-vs-rest)\n",
    "            try:\n",
    "                results.append(roc_auc_score(all_labels, all_probs, multi_class='ovr'))\n",
    "            except:\n",
    "                results.append(0.0)\n",
    "        if param == \"recall\":\n",
    "            results.append(recall_score(all_labels, all_preds, average='macro'))\n",
    "        if param == \"precision\":\n",
    "            results.append(precision_score(all_labels, all_preds, average='macro'))\n",
    "        if param == \"fmeasure\":\n",
    "            precision = precision_score(all_labels, all_preds, average='macro')\n",
    "            recall = recall_score(all_labels, all_preds, average='macro')\n",
    "            if precision + recall > 0:\n",
    "                results.append(2 * precision * recall / (precision + recall))\n",
    "            else:\n",
    "                results.append(0.0)\n",
    "    \n",
    "    model.train()\n",
    "    return results, all_preds, all_labels\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, save_path, title):\n",
    "    \"\"\"Confusion matrix를 class별 accuracy (확률)로 표시하고 저장\"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Row-wise normalization: 각 class별로 맞춘 확률 (recall per class)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    cm_normalized = np.nan_to_num(cm_normalized)  # NaN 처리\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Heatmap with normalized values\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                ax=ax, vmin=0, vmax=1, cbar_kws={'label': 'Accuracy'})\n",
    "    \n",
    "    # Add raw counts as secondary annotation\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            ax.text(j + 0.5, i + 0.75, f'({cm[i, j]})', \n",
    "                   ha='center', va='center', fontsize=8, color='gray')\n",
    "    \n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return cm_normalized\n",
    "\n",
    "\n",
    "def get_patch_time_ms(patch_idx, time_bin, sampling_rate, start_time_ms=-200):\n",
    "    \"\"\"패치 인덱스를 실제 시간(ms)으로 변환\"\"\"\n",
    "    time_per_bin_ms = time_bin / sampling_rate * 1000\n",
    "    start_ms = patch_idx * time_per_bin_ms + start_time_ms\n",
    "    return start_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOAD(traer mis archivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bin = 32\n",
    "batch_size = 64\n",
    "MODEL_INPUT_LEN = 448\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate random data\n",
    "\n",
    "##### Data format:\n",
    "Datatype - float32 (both X and Y) <br>\n",
    "X.shape - (#samples, 1, #timepoints,  #channels) <br>\n",
    "Y.shape - (#samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train = np.random.rand(100, 1, 120, 64).astype('float32') # np.random.rand generates between [0, 1)\n",
    "# y_train = np.round(np.random.rand(100).astype('float32')) # binary data, so we round it to 0 or 1.\n",
    "\n",
    "# X_val = np.random.rand(100, 1, 120, 64).astype('float32')\n",
    "# y_val = np.round(np.random.rand(100).astype('float32'))\n",
    "\n",
    "# X_test = np.random.rand(100, 1, 120, 64).astype('float32')\n",
    "# y_test = np.round(np.random.rand(100).astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class COMBLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, files, sampling_rate=256, is_train=True, is_augment=False, patch_idx=None, time_bin=None):\n",
    "        self.root = root\n",
    "        self.files = files\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.is_train = is_train\n",
    "        self.is_augment = is_augment\n",
    "        self.patch_idx = patch_idx\n",
    "\n",
    "        self.model_input_len = MODEL_INPUT_LEN\n",
    "        if time_bin is None:\n",
    "            self.time_bin = int(self.sampling_rate * 0.05)\n",
    "        else:\n",
    "            self.time_bin = time_bin\n",
    "\n",
    "        self.total_blocks = self.model_input_len // self.time_bin\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = pickle.load(open(os.path.join(self.root, self.files[index]), \"rb\"))\n",
    "        X = sample[\"signal\"]  # shape: (n_channels, data_len)\n",
    "        Y = int(sample[\"label\"] - 1)  # [2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4]\n",
    "        \n",
    "        n_channels = X.shape[0]\n",
    "        data_len = X.shape[-1]\n",
    "        model_input_len = self.model_input_len\n",
    "\n",
    "        # z-score normalization\n",
    "        mean = np.mean(X, axis=-1, keepdims=True)\n",
    "        std = np.std(X, axis=-1, keepdims=True) + 1e-6\n",
    "        X = (X - mean) / std\n",
    "\n",
    "        time_bin = self.time_bin\n",
    "        max_valid_patch = data_len // time_bin\n",
    "\n",
    "        # 1. random window augmentation\n",
    "        if self.is_augment and data_len > time_bin:\n",
    "            if random.random() < 0.5:\n",
    "                # cumulative Window Augmentation\n",
    "                end_patch_idx = random.randint(1, max_valid_patch)\n",
    "                start_patch_idx = 0\n",
    "            else:\n",
    "                # random window augmentation\n",
    "                end_patch_idx = random.randint(1, max_valid_patch)\n",
    "                start_patch_idx = random.randint(0, end_patch_idx)\n",
    "        # 2. Fixed window\n",
    "        else:\n",
    "            if self.patch_idx is not None:\n",
    "                # 지정된 패치 인덱스 사용\n",
    "                start_patch_idx = self.patch_idx\n",
    "                end_patch_idx = self.patch_idx + 1  # 끝 인덱스는 포함되지 않으므로 +1\n",
    "            else:\n",
    "                # [Test] 전체 사용\n",
    "                start_patch_idx = 0\n",
    "                end_patch_idx = max_valid_patch\n",
    "\n",
    "        # 전체를 0으로 초기화 (보고자 하는 영역만 값이 들어감)\n",
    "        input_tensor = torch.zeros((n_channels, model_input_len), dtype=torch.float32)\n",
    "        mask = torch.zeros(model_input_len, dtype=torch.float32)\n",
    "\n",
    "        # 유효 구간 계산\n",
    "        start_t_index = start_patch_idx * time_bin\n",
    "        end_t_index = min(end_patch_idx * time_bin, data_len, model_input_len)\n",
    "\n",
    "        # 해당 구간만 데이터 복사 (나머지는 0)\n",
    "        copy_len = min(end_t_index - start_t_index, model_input_len - start_t_index)\n",
    "        if copy_len > 0:\n",
    "            input_tensor[:, start_t_index:start_t_index + copy_len] = torch.from_numpy(\n",
    "                X[:, start_t_index:start_t_index + copy_len].astype(np.float32)\n",
    "            )\n",
    "            mask[start_t_index:start_t_index + copy_len] = 1.0\n",
    "\n",
    "        # Shape 변환: (C, T) -> (1, T, C) for EEGNet input\n",
    "        # EEGNet expects: (batch, 1, T, C)\n",
    "        input_tensor = input_tensor.transpose(0, 1)  # (T, C)\n",
    "        input_tensor = input_tensor.unsqueeze(0)     # (1, T, C)\n",
    "\n",
    "        return input_tensor, Y, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/local_raid3/03_user/myyu/EEGPT/downstream_combine3/PreprocessedEEG\"\n",
    "train_files = os.listdir(os.path.join(root_dir, \"processed_train\"))\n",
    "test_files = os.listdir(os.path.join(root_dir, \"processed_test\"))\n",
    "\n",
    "print(\"Loading train dataset...\")\n",
    "# Training dataset (augmentation 사용)\n",
    "train_dataset = COMBLoader(\n",
    "    os.path.join(root_dir, \"processed_train\"), train_files, \n",
    "    is_train=True, is_augment=False, time_bin=time_bin\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=8, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"\\nLoading test dataset...\")\n",
    "# Test dataset (전체 데이터 사용, patch_idx=None)\n",
    "test_dataset = COMBLoader(\n",
    "    os.path.join(root_dir, \"processed_test\"), test_files, \n",
    "    is_train=False, time_bin=time_bin\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, num_workers=8, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# 데이터 shape 및 레이블 확인\n",
    "sample_input, sample_label, sample_mask = train_dataset[0]\n",
    "print(f\"Sample input shape: {sample_input.shape}\")  # (1, 448, 53) expected\n",
    "print(f\"Sample label (model input, 0-4): {sample_label}\")\n",
    "print(f\"Sample label (display, 1-5): {sample_label + 1}\")\n",
    "print(f\"Sample mask shape: {sample_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_dir = \"/local_raid3/03_user/myyu/EEGPT/downstream_combine3/PreprocessedEEG\"\n",
    "train_files = os.listdir(os.path.join(root_dir, \"processed_train\"))\n",
    "test_files = os.listdir(os.path.join(root_dir, \"processed_test\"))\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "num_patches = MODEL_INPUT_LEN // time_bin  # 448 // 32 = 14 patches\n",
    "sampling_rate = 500\n",
    "\n",
    "patience = 10  \n",
    "\n",
    "# 저장 디렉토리 설정\n",
    "save_dir = \"/local_raid3/03_user/myyu/EEGNet/logs\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# 클래스 이름 정의 (표시용: 0-5)\n",
    "class_names = ['0', '1', '2', '3', '4', '5']\n",
    "\n",
    "# 각 패치별로 학습 및 테스트\n",
    "for patch_idx in range(6,num_patches):\n",
    "    # 결과 저장용 딕셔너리 (패치별로 저장)\n",
    "    results = {\n",
    "        'train_patch_idx': [],\n",
    "        'time_ms': [],\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_patch_idx': [],\n",
    "        'test_acc': [],\n",
    "        'test_balanced_acc': [],\n",
    "        'test_auc': [],\n",
    "        'test_fmeasure': []\n",
    "    }\n",
    "    # 패치 시간 계산 (ms)\n",
    "    patch_time_ms = get_patch_time_ms(patch_idx, time_bin, sampling_rate, start_time_ms=-200)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with Patch {patch_idx} (Time: {patch_time_ms:.0f}ms, samples {patch_idx*time_bin}-{(patch_idx+1)*time_bin})\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    train_dir = os.path.join(save_dir, f\"patch_{patch_idx}\")\n",
    "    cm_dir = os.path.join(train_dir, \"cm\")\n",
    "    os.makedirs(cm_dir, exist_ok=True)\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "    # 해당 패치만 사용하는 train dataset\n",
    "    train_dataset_patch = COMBLoader(\n",
    "        os.path.join(root_dir, \"processed_train\"), train_files,\n",
    "        is_train=True, is_augment=False, time_bin=time_bin, patch_idx=patch_idx\n",
    "    )\n",
    "    train_loader_patch = torch.utils.data.DataLoader(\n",
    "        train_dataset_patch, batch_size=batch_size, num_workers=8, shuffle=True, persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    # 모델 재초기화 (각 패치마다 새로운 모델)\n",
    "    net = EEGNet(n_channels=53, n_timepoints=448, n_classes=6).cuda(0)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_model_path = None\n",
    "    \n",
    "    # 학습\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels, mask in train_loader_patch:\n",
    "            inputs = inputs.cuda(0)\n",
    "            labels = labels.cuda(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_acc = 100 * correct / total\n",
    "        avg_loss = running_loss / len(train_loader_patch)\n",
    "        \n",
    "        # 테스트 및 모델 저장\n",
    "        test_results, all_preds, all_labels = evaluate(net, test_loader_all, [\"acc\"])\n",
    "        current_acc = test_results[0] * 100\n",
    "        \n",
    "        if current_acc > best_acc:\n",
    "            best_acc = current_acc\n",
    "            early_stop_counter = 0  # Early stopping counter reset\n",
    "\n",
    "            file_name = f\"EEGNet_F1_P{patch_idx}_CAll-epoch={epoch+1}-valid_acc={current_acc:.2f}.pth\"\n",
    "            best_model_path = os.path.join(save_dir, file_name)\n",
    "            torch.save(net.state_dict(), best_model_path)\n",
    "            \n",
    "            print(f\"  [Epoch {epoch+1}] Best Acc Updated: {current_acc:.2f}% (Loss: {avg_loss:.4f}) - Saved\")\n",
    "        \n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"  Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}, Valid Acc(1-5): {current_acc:.2f}%\")\n",
    "            \n",
    "            # Patience ���� �� ����\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"\\n[Early Stopping] Triggered at epoch {epoch+1}. No improvement for {patience} epochs.\")\n",
    "                break\n",
    "    # -----------------------------------------------------\n",
    "    # 학습 종료 후: Best Model 로드 및 최종 평가 (CM 저장)\n",
    "    # -----------------------------------------------------\n",
    "    if best_model_path:\n",
    "        torch.cuda.empty_cache()\n",
    "        net.load_state_dict(torch.load(best_model_path))\n",
    "        print(f\"Loaded Best Model: {os.path.basename(best_model_path)}\")\n",
    "    \n",
    "    for test_patch_idx in range(num_patches):\n",
    "        print(f\"  Evaluating on Test Patch {test_patch_idx}...\")\n",
    "        # 해당 패치만 사용하는 test dataset\n",
    "        test_dataset_patch = COMBLoader(\n",
    "            os.path.join(root_dir, \"processed_test\"), test_files,\n",
    "            is_train=False, time_bin=time_bin, patch_idx=test_patch_idx\n",
    "        )\n",
    "        test_loader_patch = torch.utils.data.DataLoader(\n",
    "            test_dataset_patch, batch_size=batch_size, num_workers=0, shuffle=False\n",
    "        )\n",
    "\n",
    "\n",
    "        # 최종 평가 (Metrics 계산)\n",
    "        test_metrics, all_preds, all_labels = evaluate(net, test_loader_patch, [\"acc\", \"auc\", \"fmeasure\"])\n",
    "\n",
    "\n",
    "\n",
    "        # 0번 클래스 제외 필터링 (Confusion Matrix용)\n",
    "        np_preds = np.array(all_preds)\n",
    "        np_labels = np.array(all_labels)\n",
    "        valid_indices = np.where(np_labels != 0)[0]\n",
    "        \n",
    "        filtered_preds = np_preds[valid_indices]\n",
    "        filtered_labels = np_labels[valid_indices]\n",
    "        \n",
    "        final_acc = 100 * (filtered_preds == filtered_labels).sum() / len(valid_indices)\n",
    "        final_balanced_acc = 100 * balanced_accuracy_score(filtered_labels, filtered_preds)\n",
    "\n",
    "        # CM 저장\n",
    "        cm_filename = f\"P{patch_idx}_testP{test_patch_idx}_Acc{final_acc:.2f}_Bal{final_balanced_acc:.2f}_cm.png\"\n",
    "        cm_save_path = os.path.join(train_dir, cm_filename)\n",
    "        cm_title = f\"Train Patch {patch_idx}({patch_time_ms:.0f}ms)/Test Patch {test_patch_idx} \\nAcc: {final_acc:.2f}% | Bal Acc: {final_balanced_acc:.2f}%\"\n",
    "        display_class_names = ['1', '2', '3', '4', '5']\n",
    "        \n",
    "        plot_confusion_matrix(filtered_labels, filtered_preds, display_class_names, cm_save_path, cm_title)\n",
    "        print(f\"  Confusion matrix saved: {cm_save_path}\")\n",
    "        plt.close('all') # 모든 figure 닫기\n",
    "\n",
    "        # 결과 기록\n",
    "        results['train_patch_idx'].append(patch_idx)\n",
    "        results['time_ms'].append(patch_time_ms)\n",
    "        results['train_loss'].append(avg_loss) # 마지막 epoch loss\n",
    "        results['train_acc'].append(train_acc) # 마지막 epoch train acc\n",
    "        results['test_patch_idx'].append(test_patch_idx)\n",
    "        results['test_acc'].append(final_acc)\n",
    "        results['test_balanced_acc'].append(final_balanced_acc) # Balanced Acc\n",
    "        results['test_auc'].append(test_metrics[1])\n",
    "        results['test_fmeasure'].append(test_metrics[2])\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. 전체 결과 저장 및 그래프 시각화\n",
    "    # ==========================================\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Saving Results & Graphs...\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # 1) DataFrame으로 변환 및 CSV 저장\n",
    "    df_results = pd.DataFrame(results)\n",
    "    csv_filename = f\"P{patch_idx}_testP{test_patch_idx}_Acc{final_acc:.2f}_Bal{final_balanced_acc:.2f}_results.csv\"\n",
    "    csv_path = os.path.join(train_dir, csv_filename)\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    print(f\"Results saved to: {csv_path}\")\n",
    "\n",
    "    # 2) 전체 Test Accuracy 그래프 그리기\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Standard Accuracy Plot\n",
    "    plt.plot(df_results['test_patch_idx'], df_results['test_acc'], \n",
    "            marker='o', label='Test Accuracy', color='blue', linewidth=2)\n",
    "\n",
    "    # Balanced Accuracy Plot (점선으로 표시)\n",
    "    plt.plot(df_results['test_patch_idx'], df_results['test_balanced_acc'], \n",
    "            marker='s', label='Balanced Accuracy', color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "    plt.title(\"Test Accuracy per Time Patch\", fontsize=15)\n",
    "    plt.xlabel(\"Time (ms)\", fontsize=12)\n",
    "    plt.ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "    plt.ylim(0, 100) # 0~100% 범위 고정\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # X축 틱 설정 (가독성을 위해)\n",
    "    plt.xticks(df_results['test_patch_idx'], rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    graph_filename = f\"P{patch_idx}_testP{test_patch_idx}_Acc{final_acc:.2f}_Bal{final_balanced_acc:.2f}_acc.png\"\n",
    "    graph_path = os.path.join(save_dir, graph_filename)\n",
    "    plt.savefig(graph_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Graph saved to: {graph_path}\")\n",
    "    print(\"All tasks finished successfully.\")\n",
    "    del net, optimizer, criterion, train_loader_patch\n",
    "    if 'test_loader_patch' in locals(): del test_loader_patch\n",
    "    torch.cuda.empty_cache() # GPU ������ ������\n",
    "    gc.collect() # Python 메모리 캐시 비우기\n",
    "\n",
    "    print(f\"Finished Patch {patch_idx}. Moving to next...\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All Patches Training Complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 결과를 DataFrame으로 변환\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# X축 레이블 생성 (5개 간격으로 패치 번호와 실제 시간 표시)\n",
    "tick_interval = 5\n",
    "tick_indices = list(range(0, num_patches, tick_interval))\n",
    "if (num_patches - 1) not in tick_indices:\n",
    "    tick_indices.append(num_patches - 1)\n",
    "tick_labels = [f\"P{i}\\n({df_results['time_ms'].iloc[i]:.0f}ms)\" for i in tick_indices]\n",
    "\n",
    "# 결과 시각화\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Test Accuracy per Patch (시간 레이블)\n",
    "axes[0, 0].plot(df_results['patch_idx'], df_results['test_acc'], 'o-', color='steelblue', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_xlabel('Patch Index (Time in ms)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Test Accuracy (%)', fontsize=11)\n",
    "axes[0, 0].set_title('Test Accuracy by Training Patch', fontsize=13)\n",
    "axes[0, 0].set_xticks(tick_indices)\n",
    "axes[0, 0].set_xticklabels(tick_labels, fontsize=9)\n",
    "axes[0, 0].axhline(y=df_results['test_acc'].mean(), color='r', linestyle='--', \n",
    "                   label=f\"Avg: {df_results['test_acc'].mean():.2f}%\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Test AUC per Patch\n",
    "axes[0, 1].plot(df_results['patch_idx'], df_results['test_auc'], 's-', color='coral', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_xlabel('Patch Index (Time in ms)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Test AUC', fontsize=11)\n",
    "axes[0, 1].set_title('Test AUC by Training Patch', fontsize=13)\n",
    "axes[0, 1].set_xticks(tick_indices)\n",
    "axes[0, 1].set_xticklabels(tick_labels, fontsize=9)\n",
    "axes[0, 1].axhline(y=df_results['test_auc'].mean(), color='r', linestyle='--',\n",
    "                   label=f\"Avg: {df_results['test_auc'].mean():.4f}\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Train Accuracy per Patch\n",
    "axes[1, 0].plot(df_results['patch_idx'], df_results['train_acc'], '^-', color='green', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_xlabel('Patch Index (Time in ms)', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Train Accuracy (%)', fontsize=11)\n",
    "axes[1, 0].set_title('Final Train Accuracy by Patch', fontsize=13)\n",
    "axes[1, 0].set_xticks(tick_indices)\n",
    "axes[1, 0].set_xticklabels(tick_labels, fontsize=9)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Train vs Test Accuracy\n",
    "axes[1, 1].plot(df_results['patch_idx'], df_results['train_acc'], '^-', color='green', \n",
    "                linewidth=2, markersize=7, label='Train Acc', alpha=0.8)\n",
    "axes[1, 1].plot(df_results['patch_idx'], df_results['test_acc'], 'o-', color='steelblue', \n",
    "                linewidth=2, markersize=7, label='Test Acc', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Patch Index (Time in ms)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Accuracy (%)', fontsize=11)\n",
    "axes[1, 1].set_title('Train vs Test Accuracy by Patch', fontsize=13)\n",
    "axes[1, 1].set_xticks(tick_indices)\n",
    "axes[1, 1].set_xticklabels(tick_labels, fontsize=9)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Accuracy 그래프 저장\n",
    "acc_graph_path = os.path.join(save_dir, 'patch_training_accuracy.png')\n",
    "plt.savefig(acc_graph_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nAccuracy graph saved: {acc_graph_path}\")\n",
    "plt.show()\n",
    "\n",
    "# 최종 결과 요약\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Results Summary\")\n",
    "print(\"=\"*60)\n",
    "best_idx = df_results['test_acc'].idxmax()\n",
    "worst_idx = df_results['test_acc'].idxmin()\n",
    "print(f\"\\nBest Patch: P{df_results.loc[best_idx, 'patch_idx']} (Time: {df_results.loc[best_idx, 'time_ms']:.0f}ms) \"\n",
    "      f\"- Test Acc: {df_results['test_acc'].max():.2f}%\")\n",
    "print(f\"Worst Patch: P{df_results.loc[worst_idx, 'patch_idx']} (Time: {df_results.loc[worst_idx, 'time_ms']:.0f}ms) \"\n",
    "      f\"- Test Acc: {df_results['test_acc'].min():.2f}%\")\n",
    "print(f\"\\nAverage Test Acc: {df_results['test_acc'].mean():.2f}% (std: {df_results['test_acc'].std():.2f}%)\")\n",
    "print(f\"Average Test AUC: {df_results['test_auc'].mean():.4f} (std: {df_results['test_auc'].std():.4f})\")\n",
    "print(f\"Average Test F1:  {df_results['test_fmeasure'].mean():.4f} (std: {df_results['test_fmeasure'].std():.4f})\")\n",
    "\n",
    "# 결과 CSV로 저장\n",
    "csv_path = os.path.join(save_dir, 'patch_training_results.csv')\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"\\nResults saved to: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eegnet310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
